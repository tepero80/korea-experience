"""
ë‚´ë¶€ ë§í¬ ê´€ë¦¬ ëª¨ë“ˆ
====================
í¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ë¥¼ ë¹Œë“œí•˜ê³ , ì£¼ì–´ì§„ ì£¼ì œì— ê°€ì¥ ê´€ë ¨ ë†’ì€ í¬ìŠ¤íŠ¸ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
Deep-dive í¬ìŠ¤íŠ¸ë¥¼ ìš°ì„  ì¶”ì²œí•˜ê³ , ë¶€ì¡±í•˜ë©´ ì¼ë°˜ í¬ìŠ¤íŠ¸ë¡œ ë³´ì¶©í•©ë‹ˆë‹¤.

ì‚¬ìš©:
  from scripts.deep_dive.links import build_index, recommend_links

ì¸ë±ìŠ¤ ìºì‹œ:
  scripts/deep_dive/link-index.json  (ìë™ ìƒì„±/ê°±ì‹ )
"""

import json
import re
import time
from pathlib import Path

from .config import (
    API_KEY, SCENE_MODEL,        # Flash ëª¨ë¸ â€” ë¹ ë¥´ê³  ì €ë ´
    DEEP_DIVE_DIR, POSTS_DIR,
    DEEP_DIVE_SCRIPTS,
)

INDEX_FILE = DEEP_DIVE_SCRIPTS / "link-index.json"

# â”€â”€ ì¸ë±ìŠ¤ ë¹Œë“œ â”€â”€

def _extract_frontmatter(filepath: Path) -> dict | None:
    """MD íŒŒì¼ì—ì„œ frontmatterë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    try:
        text = filepath.read_text(encoding="utf-8-sig")
    except Exception:
        try:
            text = filepath.read_text(encoding="cp949")
        except Exception:
            return None

    fm_match = re.match(r"^---\s*\n(.*?)\n---", text, re.DOTALL)
    if not fm_match:
        return None
    fm = fm_match.group(1)

    title_m = re.search(r'title:\s*"?([^"\n]+)"?', fm)
    cat_m = re.search(r'category:\s*(.+)', fm)
    excerpt_m = re.search(r'excerpt:\s*"?([^"\n]+)"?', fm)
    deep_m = re.search(r'deepDive:\s*true', fm, re.IGNORECASE)

    if not title_m:
        return None

    title = title_m.group(1).strip()

    # í‚¤ì›Œë“œ ì¶”ì¶œ: ì œëª©ì—ì„œ ë¶ˆìš©ì–´ ì œê±° í›„ 3ê¸€ì ì´ìƒ ë‹¨ì–´
    stopwords = {
        "the", "and", "for", "how", "what", "why", "with", "your", "from",
        "that", "this", "are", "was", "will", "can", "all", "our", "you",
        "best", "top", "guide", "complete", "ultimate", "2026", "2025",
        "korea", "korean", "seoul",
    }
    keywords = [
        w.lower() for w in re.findall(r'[a-zA-Z]+', title)
        if len(w) >= 3 and w.lower() not in stopwords
    ]

    return {
        "slug": filepath.stem,
        "title": title,
        "category": cat_m.group(1).strip() if cat_m else "",
        "excerpt": excerpt_m.group(1).strip()[:200] if excerpt_m else "",
        "keywords": keywords,
        "deep_dive": bool(deep_m),
    }


def build_index(force: bool = False) -> list[dict]:
    """ì „ì²´ í¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ë¥¼ ë¹Œë“œí•˜ê³  JSONìœ¼ë¡œ ìºì‹œí•©ë‹ˆë‹¤.

    Args:
        force: Trueë©´ ìºì‹œ ë¬´ì‹œí•˜ê³  ì¬ë¹Œë“œ

    Returns:
        [{"slug", "title", "category", "excerpt", "keywords", "deep_dive"}]
    """
    # ìºì‹œ ì²´í¬: íŒŒì¼ì´ ìˆê³  1ì‹œê°„ ì´ë‚´ë©´ ì¬ì‚¬ìš©
    if not force and INDEX_FILE.exists():
        age = time.time() - INDEX_FILE.stat().st_mtime
        if age < 3600:
            data = json.loads(INDEX_FILE.read_text(encoding="utf-8"))
            return data

    index = []

    # Deep-dive í¬ìŠ¤íŠ¸ (ìš°ì„ )
    if DEEP_DIVE_DIR.exists():
        for f in sorted(DEEP_DIVE_DIR.glob("*.md")):
            entry = _extract_frontmatter(f)
            if entry:
                index.append(entry)

    # ì¼ë°˜ í¬ìŠ¤íŠ¸
    if POSTS_DIR.exists():
        for f in sorted(POSTS_DIR.glob("*.md")):
            if f.suffix == ".bak":
                continue
            entry = _extract_frontmatter(f)
            if entry:
                index.append(entry)

    # ìºì‹œ ì €ì¥
    INDEX_FILE.write_text(
        json.dumps(index, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )
    print(f"ğŸ“‡ ë§í¬ ì¸ë±ìŠ¤ ë¹Œë“œ: {sum(1 for e in index if e['deep_dive'])} deep-dive + "
          f"{sum(1 for e in index if not e['deep_dive'])} posts = {len(index)}ê°œ")

    return index


def refresh_index():
    """ìºì‹œë¥¼ ê°•ì œ ì¬ë¹Œë“œí•©ë‹ˆë‹¤."""
    return build_index(force=True)


# â”€â”€ í‚¤ì›Œë“œ ê¸°ë°˜ ì‚¬ì „ í•„í„° â”€â”€

def _keyword_score(entry: dict, topic_keywords: list[str]) -> int:
    """í‚¤ì›Œë“œ ê²¹ì¹¨ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    entry_kw = set(entry["keywords"])
    topic_kw = set(topic_keywords)
    return len(entry_kw & topic_kw)


def _prefilter(index: list[dict], topic: str, category: str,
               exclude_slug: str = "", top_n: int = 40) -> list[dict]:
    """í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ í›„ë³´ë¥¼ ì‚¬ì „ í•„í„°ë§í•©ë‹ˆë‹¤."""
    stopwords = {
        "the", "and", "for", "how", "what", "why", "with", "your", "from",
        "that", "this", "are", "was", "will", "can", "all", "our", "you",
        "best", "top", "guide", "complete", "ultimate", "2026", "2025",
        "korea", "korean", "seoul",
    }
    topic_keywords = [
        w.lower() for w in re.findall(r'[a-zA-Z]+', topic)
        if len(w) >= 3 and w.lower() not in stopwords
    ]

    scored = []
    for entry in index:
        if entry["slug"] == exclude_slug:
            continue
        kw_score = _keyword_score(entry, topic_keywords)
        # ê°™ì€ ì¹´í…Œê³ ë¦¬ ë³´ë„ˆìŠ¤
        cat_bonus = 2 if entry["category"] == category else 0
        # deep-dive ë³´ë„ˆìŠ¤
        dd_bonus = 3 if entry["deep_dive"] else 0
        scored.append((kw_score + cat_bonus + dd_bonus, entry))

    scored.sort(key=lambda x: x[0], reverse=True)
    return [e for _, e in scored[:top_n]]


# â”€â”€ Gemini ê¸°ë°˜ ì¶”ì²œ â”€â”€

RECOMMEND_PROMPT = """You are an internal link strategist for "Korea Experience", a Korea travel blog.

Given a new article topic and a list of existing posts, select the 5-8 most relevant posts to link FROM the new article.

RULES:
1. Prioritize [DEEP DIVE] posts â€” they are flagship content
2. Choose posts that a reader of the new article would ACTUALLY want to click
3. Prefer topical overlap (same category, related subject)
4. Include at least 1-2 posts from different categories for cross-linking
5. Return ONLY the slugs, one per line, most relevant first

NEW ARTICLE:
  Topic: {topic}
  Category: {category}

CANDIDATE POSTS:
{candidates}

OUTPUT (slugs only, one per line, 5-8 lines):"""


def recommend_links(topic: str, category: str, slug: str = "",
                    api_key: str = "", count: int = 8) -> list[dict]:
    """ì£¼ì–´ì§„ ì£¼ì œì— ê°€ì¥ ê´€ë ¨ ë†’ì€ í¬ìŠ¤íŠ¸ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.

    Deep-dive ìš°ì„ , ë¶€ì¡±í•˜ë©´ ì¼ë°˜ í¬ìŠ¤íŠ¸ë¡œ ë³´ì¶©í•©ë‹ˆë‹¤.

    Args:
        topic: ìƒˆ ê¸€ ì£¼ì œ
        category: ì¹´í…Œê³ ë¦¬ëª…
        slug: ìê¸° ìì‹ ì˜ ìŠ¬ëŸ¬ê·¸ (ì œì™¸ìš©)
        api_key: Gemini API key
        count: ì¶”ì²œ ê°œìˆ˜

    Returns:
        [{"slug", "title", "deep_dive"}] â€” ì¶”ì²œ ìˆœì„œëŒ€ë¡œ
    """
    index = build_index()

    if not index:
        print("âš ï¸  ë§í¬ ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return []

    # 1ë‹¨ê³„: í‚¤ì›Œë“œ ì‚¬ì „ í•„í„° â†’ ìƒìœ„ 40ê°œ í›„ë³´
    candidates = _prefilter(index, topic, category, exclude_slug=slug, top_n=40)

    if not candidates:
        return []

    # í›„ë³´ ëª©ë¡ í¬ë§·
    candidate_lines = []
    for c in candidates:
        tag = "[DEEP DIVE] " if c["deep_dive"] else ""
        candidate_lines.append(f"  {tag}{c['slug']}  â€”  {c['title']}  ({c['category']})")
    candidates_text = "\n".join(candidate_lines)

    # 2ë‹¨ê³„: Gemini Flashë¡œ ìµœì¢… ì„ ë³„
    api_key = api_key or API_KEY
    if not api_key:
        # API ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ì ìˆ˜ ê¸°ë°˜ fallback
        print("âš ï¸  API key ì—†ìŒ â†’ í‚¤ì›Œë“œ ì ìˆ˜ ê¸°ë°˜ ì¶”ì²œ")
        return _fallback_recommend(candidates, count)

    try:
        from google import genai
        from google.genai import types

        client = genai.Client(api_key=api_key)
        prompt = RECOMMEND_PROMPT.format(
            topic=topic, category=category, candidates=candidates_text,
        )

        response = client.models.generate_content(
            model=SCENE_MODEL,  # gemini-2.5-flash â€” ë¹ ë¥´ê³  ì €ë ´
            contents=[prompt],
            config=types.GenerateContentConfig(temperature=0.2, max_output_tokens=1024),
        )

        result_text = response.text.strip()
        recommended_slugs = [
            line.strip() for line in result_text.splitlines()
            if line.strip() and not line.strip().startswith("#")
        ]

        # ìŠ¬ëŸ¬ê·¸ë¡œ ì¸ë±ìŠ¤ì—ì„œ ì°¾ê¸°
        slug_map = {e["slug"]: e for e in index}
        recommended = []
        for rs in recommended_slugs[:count]:
            # ì •í™• ë§¤ì¹­
            if rs in slug_map:
                recommended.append(slug_map[rs])
                continue
            # ë¶€ë¶„ ë§¤ì¹­ (LLMì´ ì•½ê°„ ë‹¤ë¥´ê²Œ ì¶œë ¥í•  ìˆ˜ ìˆìŒ)
            clean = rs.strip("- ").strip()
            if clean in slug_map:
                recommended.append(slug_map[clean])

        if len(recommended) < 3:
            # LLM ê²°ê³¼ ë¶€ì¡± â†’ fallback ë³´ì¶©
            existing = {r["slug"] for r in recommended}
            for c in candidates:
                if c["slug"] not in existing:
                    recommended.append(c)
                    if len(recommended) >= count:
                        break

        print(f"ğŸ”— ë‚´ë¶€ ë§í¬ ì¶”ì²œ {len(recommended)}ê°œ "
              f"(deep-dive: {sum(1 for r in recommended if r['deep_dive'])})")
        return recommended[:count]

    except Exception as e:
        print(f"âš ï¸  Gemini ì¶”ì²œ ì‹¤íŒ¨: {e} â†’ í‚¤ì›Œë“œ ê¸°ë°˜ fallback")
        return _fallback_recommend(candidates, count)


def _fallback_recommend(candidates: list[dict], count: int) -> list[dict]:
    """API ì—†ì´ í‚¤ì›Œë“œ ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤. Deep-dive ìš°ì„ ."""
    # deep-dive ë¨¼ì €, ì¼ë°˜ í¬ìŠ¤íŠ¸ ë’¤ì—
    dd = [c for c in candidates if c["deep_dive"]]
    normal = [c for c in candidates if not c["deep_dive"]]

    result = dd[:count]
    remaining = count - len(result)
    if remaining > 0:
        result.extend(normal[:remaining])

    print(f"ğŸ”— ë‚´ë¶€ ë§í¬ fallback ì¶”ì²œ {len(result)}ê°œ "
          f"(deep-dive: {sum(1 for r in result if r['deep_dive'])})")
    return result[:count]


# â”€â”€ í”„ë¡¬í”„íŠ¸ìš© í¬ë§· â”€â”€

def format_links_for_prompt(recommended: list[dict]) -> str:
    """MDX ë³€í™˜ í”„ë¡¬í”„íŠ¸ì— ë„£ì„ ì¶”ì²œ ë§í¬ ì„¹ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if not recommended:
        return ""

    lines = ["RECOMMENDED INTERNAL LINKS (use 3-5 of these, prioritize â˜… deep-dive posts):"]
    for r in recommended:
        star = "â˜… " if r["deep_dive"] else "  "
        lines.append(f"  {star}/blog/{r['slug']}  â€”  \"{r['title']}\"")

    lines.append("")
    lines.append("Insert links naturally in context using [descriptive anchor text](/blog/slug) format.")
    lines.append("Do NOT dump all links in one section. Spread them across relevant paragraphs.")

    return "\n".join(lines)
